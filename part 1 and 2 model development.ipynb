{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence \n",
    "from functools import partial\n",
    "import random # to generate random numbers\n",
    "import torch # to build the model\n",
    "import numpy as np # to perform calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Ensures reproducibility across runs, all the randoms generated will have the same seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    \"\"\"\n",
    "    Generates random integer-encoded DNA sequences of fixed length (default 128). \n",
    "    Each int from 0 to 4 corresponds to one of: 'N', 'A', 'C', 'G', 'T'.\n",
    "    Example: [0, 2, 3, 1, 4, 3] might represent \"NCGA TG\"\n",
    "    \"\"\"\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts how many \"CG\" pairs are in the DNA string. This is my regression label (target output). It is done using \n",
    "    programmable logic, which I will need to do using deep learning.\n",
    "    \"\"\"\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "\"\"\"\n",
    "Allows converting between integer-sequence form and string-sequence form.\n",
    "\"\"\"\n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "# we prepared two datasets for training and evaluation\n",
    "# training data scale we set to 2048\n",
    "# we test on 512\n",
    "\n",
    "def prepare_data(num_samples=100):\n",
    "    \"\"\"\n",
    "    Create dataset by:\n",
    "        Generating X (random DNA int sequences)\n",
    "        Converting to string DNA (temp)\n",
    "        Getting labels y using count_cpgs(temp)\n",
    "    \"\"\"\n",
    "    # prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    # step 1\n",
    "    X_dna_seqs_train = list(rand_sequence(num_samples))\n",
    "    \"\"\"\n",
    "    hint:\n",
    "        1. You can check X_dna_seqs_train by print, the data is ids which is your training X \n",
    "        2. You first convert ids back to DNA sequence\n",
    "        3. Then you run count_cpgs which will yield CGs counts - this will be the labels (Y)\n",
    "    \"\"\"\n",
    "    ## Step 2: Convert each int sequence back to string using int2dna\n",
    "    temp = [''.join(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train]\n",
    "\n",
    "    # Step 3: Count CG pairs in each DNA string to form target labels\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in temp]\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Model and training hyperparameters\n",
    "LSTM_HIDDEN = 64\n",
    "LSTM_LAYER = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epoch_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_tensor_x = torch.tensor(train_x, dtype=torch.long)\n",
    "train_tensor_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "\n",
    "test_tensor_x = torch.tensor(test_x, dtype=torch.long)\n",
    "test_tensor_y = torch.tensor(test_y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for batching\n",
    "train_data_loader = DataLoader(TensorDataset(train_tensor_x, train_tensor_y), batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(TensorDataset(test_tensor_x, test_tensor_y), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        # TODO complete model, you are free to add whatever layers you need here\n",
    "        # We do need a lstm and a classifier layer here but you are free to implement them in your way\n",
    "        \n",
    "        super(CpGPredictor, self).__init__()\n",
    "        # Convert int (0–4) to 16-dim embedding vector\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=5, embedding_dim=16)\n",
    "        # LSTM for sequence modeling\n",
    "        self.lstm = torch.nn.LSTM(input_size=16, hidden_size=LSTM_HIDDEN, num_layers=LSTM_LAYER, batch_first=True)\n",
    "        # Final linear layer to output 1 regression value\n",
    "        self.classifier = torch.nn.Linear(LSTM_HIDDEN, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                   # [batch, seq_len, embed_dim]\n",
    "        lstm_out, _ = self.lstm(x)              # [batch, seq_len, hidden_size]\n",
    "        last_hidden = lstm_out[:, -1, :]        # Take the last time step\n",
    "        logits = self.classifier(last_hidden)   # Output: [batch, 1]\n",
    "        return logits.squeeze(1)                # Remove extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "\"\"\"\n",
    "Initialize the model, loss, and optimizer.\n",
    "\"\"\"\n",
    "model = CpGPredictor()\n",
    "loss_fn = torch.nn.MSELoss()                            # Good for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Training Loss: 575.4057\n",
      "Epoch 2/10 - Training Loss: 136.6816\n",
      "Epoch 3/10 - Training Loss: 134.1474\n",
      "Epoch 4/10 - Training Loss: 134.2021\n",
      "Epoch 5/10 - Training Loss: 134.4268\n",
      "Epoch 6/10 - Training Loss: 134.2547\n",
      "Epoch 7/10 - Training Loss: 134.1814\n",
      "Epoch 8/10 - Training Loss: 134.2760\n",
      "Epoch 9/10 - Training Loss: 134.2533\n",
      "Epoch 10/10 - Training Loss: 134.2186\n"
     ]
    }
   ],
   "source": [
    "# training (you can modify the code below)\n",
    "t_loss = 0.0\n",
    "\n",
    "\"\"\"\n",
    "Train the model across epochs.\n",
    "    I need to complete:\n",
    "        Fetch inputs, targets from batch\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward(); optimizer.step(); optimizer.zero_grad()\n",
    "\"\"\"\n",
    "for epoch in range(epoch_num):\n",
    "    t_loss = 0.0\n",
    "    for batch_x, batch_y in train_data_loader:\n",
    "        optimizer.zero_grad()                  # Reset gradients\n",
    "        preds = model(batch_x)                 # Forward pass\n",
    "        loss = loss_fn(preds, batch_y)         # Compute loss\n",
    "        loss.backward()                        # Backpropagate\n",
    "        optimizer.step()                       # Update weights\n",
    "        t_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epoch_num} - Training Loss: {t_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 2.0391\n"
     ]
    }
   ],
   "source": [
    "# eval (you can modify the code below)\n",
    "\"\"\"\n",
    "Run inference on test data.\n",
    "I need to complete:\n",
    "    Predict model(x)\n",
    "    Append predictions and ground truths to res_pred, res_gs|\n",
    "    Evaluate with RMSE or MAE\n",
    "\"\"\"\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "res_gs = []    # Ground truth labels\n",
    "res_pred = []  # Model predictions\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch_x, batch_y in test_data_loader:\n",
    "        preds = model(batch_x)  # Get model predictions\n",
    "        res_pred.extend(preds.tolist())  # Convert predictions to list and store\n",
    "        res_gs.extend(batch_y.tolist())  # Convert true labels to list and store\n",
    "\n",
    "# Simple evaluation metric: RMSE (root mean squared error)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(res_gs, res_pred))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMrRf_aVDRJm"
   },
   "source": [
    "# Part 2: what if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "\"\"\"\n",
    "These are utility functions from PyTorch used to:\n",
    "    pad_sequence: pad sequences in a batch to same length.\n",
    "    pack_padded_sequence: pack padded sequences for efficient LSTM processing.\n",
    "    pad_packed_sequence: unpack after LSTM if needed.\n",
    "\"\"\"\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AKvG-MNuXJr9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "\"\"\"\n",
    "Ensure reproducibility of random number generation.\n",
    "\"\"\"\n",
    "random.seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    \"\"\"\n",
    "    Generates sequences of random lengths between lb and ub.\n",
    "    Each element is a random integer in [1, 5] mapping to 'N', 'A', 'C', 'G', 'T'.\n",
    "    Note:\n",
    "    This time, 0 is reserved for \"pad\" token (see later), so we start from 1.\n",
    "    \"\"\"\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of \"CG\" substrings in a DNA string.\n",
    "    No change from Part 1 — it still works on a full DNA string.\n",
    "    \"\"\"\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "\"\"\"\n",
    "DNA characters ('NACGT') are now encoded as integers [1, 2, 3, 4, 5].\n",
    "Padding token is assigned to 0 (\"pad\").\n",
    "These mappings let you convert between encoded int lists and DNA strings.\n",
    "\"\"\"\n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"pad\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the task based on the change\n",
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    # TODO prepared the training and test data\n",
    "    # you need to call rand_sequence and count_cpgs here to create the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate X = variable-length DNA sequences (int-encoded)\n",
    "    Convert them to string format\n",
    "    Use count_cpgs() to generate target labels\n",
    "    \"\"\"\n",
    "    #step 1 Generate variable-length int sequences\n",
    "    X_dna_seqs_train = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    \n",
    "    # Step 2: Convert each int sequence to DNA string\n",
    "    temp = [''.join(intseq_to_dnaseq(seq)) for seq in X_dna_seqs_train]\n",
    "    \n",
    "    # Step 3: Use count_cpgs to generate targets\n",
    "    y_dna_seqs = [count_cpgs(seq) for seq in temp]\n",
    "    \n",
    "    return X_dna_seqs_train, y_dna_seqs\n",
    "\n",
    "\"\"\"\n",
    "Creates training and testing datasets of variable lengths between 64 and 128 bases.\n",
    "\"\"\"\n",
    "    \n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Defines a PyTorch Dataset to wrap your (X, y) data:\n",
    "    Returns (LongTensor(sequence), label) per sample\n",
    "    Needed for PyTorch’s DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, lists, labels) -> None:\n",
    "        self.lists = lists\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.lists[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be a collate_fn for dataloader to pad sequence  \n",
    "class PadSequence:\n",
    "    \"\"\"\n",
    "    When using variable-length sequences, PyTorch needs a custom collate_fn in the DataLoader to:\n",
    "    Pad all sequences in a batch to the length of the longest one\n",
    "    Return padded_sequences, sequence_lengths, and labels\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    def __call__(self, batch):\n",
    "        # Split batch into sequences and labels\n",
    "        sequences, labels = zip(*batch)\n",
    "\n",
    "        # Store original lengths (needed for packing)\n",
    "        lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "\n",
    "        # Pad all sequences to max length in the batch\n",
    "        padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return padded_seqs, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD DATA LOADERS WITH PAD SEQUENCE\n",
    "pad_sequence_fn = PadSequence()\n",
    "\n",
    "train_dataset = MyDataset(train_x, train_y)\n",
    "test_dataset = MyDataset(test_x, test_y)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=pad_sequence_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=pad_sequence_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE LSTM MODEL (WITH PACKED INPUT SUPPORT)\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(num_embeddings=6, embedding_dim=16, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(input_size=16, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        self.classifier = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Step 1: Embed\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Step 2: Pack the padded batch\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Step 3: LSTM\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "\n",
    "        # Step 4: Use the last hidden state of the last layer\n",
    "        final_hidden = hn[-1]\n",
    "\n",
    "        # Step 5: Predict CpG count\n",
    "        output = self.classifier(final_hidden)\n",
    "\n",
    "        return output.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE MODEL, LOSS, OPTIMIZER\n",
    "model = CpGPredictor()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 286.4987\n",
      "Epoch 2: Training Loss = 117.3417\n",
      "Epoch 3: Training Loss = 117.6175\n",
      "Epoch 4: Training Loss = 117.7837\n",
      "Epoch 5: Training Loss = 117.2906\n",
      "Epoch 6: Training Loss = 117.5815\n",
      "Epoch 7: Training Loss = 117.2316\n",
      "Epoch 8: Training Loss = 117.2386\n",
      "Epoch 9: Training Loss = 117.2249\n",
      "Epoch 10: Training Loss = 117.3609\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE MODEL\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_x, lengths, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_x, lengths)\n",
    "        loss = loss_fn(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Training Loss = {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.9889\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE THE MODEL\n",
    "model.eval()\n",
    "res_gs = []\n",
    "res_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, lengths, batch_y in test_loader:\n",
    "        preds = model(batch_x, lengths)\n",
    "        res_pred.extend(preds.tolist())\n",
    "        res_gs.extend(batch_y.tolist())\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "rmse = math.sqrt(mean_squared_error(res_gs, res_pred))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), \"cpg_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
